---
title: 'Practical machine learning: project assignment'
author: "Hicham el Bouazzaoui"
date: "8 februari 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r,results="asis", echo=FALSE}
rm(list = ls())

library(caret); 
library(kernlab); 
library(randomForest); 
library(ggplot2); 
library(e1071)
```


```{r, results = "asis", echo = FALSE}
## set correct directory
setwd("C:/Users/helbouazzaoui/Desktop/Coursera course/Data science course/Course 8 Practical machine learning/Project assignment")
```

# Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

# Load the data

Load the data and remove all NA columns to ensure a good model fit. Leave only the columns with numeric values.

```{r, echo = TRUE}

# create training and test data set
training <- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
testing <- read.csv("pml-testing.csv", na.strings = c("NA", "#DIV/0!", ""))

# exploratory data analysis
str(training, list.len = 10)
table(training$classe)
prop.table(table(training$classe))

# remove columns with administrative information
training <- training[,7:160]
testing <- testing[,7:160]

# check for columns with only NA values and remove those columns
nonNA_train <- training[,apply(!is.na(training),2,all)]
nonNA_test <- testing[,apply(!is.na(testing),2,all)]
```

# Training and test set

Create a training and a test sub set of the total training set. The training subset is used to train and create the model. The test set is to make the prediction and calculate the out of sample error. 
```{r, echo = TRUE}

# from the training set - create a training/test set
inTrain <- createDataPartition(y = nonNA_train$classe, p = 0.70, list = FALSE)

sub_train <- nonNA_train[inTrain,]
sub_test <- nonNA_train[-inTrain,]

# check correlation between the variables
M <- abs(cor(sub_train[,-54]))
diag(M) <- 0
cor_mat <- which(M > 0.8, arr.ind = T)
```


# Variable selection

Because it is not desired to use all the variables for creating the model, the most important variables have to be selected. The random forest algorithm can be used for variable selection. Using the varImPlot function and looking at the MeanDecreaseAccuracy plot, the 11 most important variables are chosen. How this is done is shown in the code below:
```{r, echo= TRUE}
# apply randomforest algorithm to perform a feature selection, to select which variables are used in the prediction model
model_RF <- randomForest(classe ~., data = sub_train, ntree=100, mtry=2, importance=TRUE)
model_RF
varImpPlot(model_RF)

# based on the MeanDecreaseAccuracy plot, select the following variables
var_selected <- c("roll_belt", "roll_arm", "pitch_belt", "accel_dumbbell_z", "num_window", "yaw_forearm", 
                  "gyros_belt_x", "yaw_arm", "magnet_dumbbell_z", "magnet_dumbbell_y", "classe")

# select only the variables which are important and add the dependent variable to the sub training set
myvars <- names(sub_train) %in% var_selected
sub_train <- sub_train[myvars]
```

From the earlier correlation matrix it is seen that roll_belt and yaw_bell are correlated. Because removing roll_belt from the 
selection means a larger decrease in accuracy, it is chosen to keep this one as a variable for the model.

# Model and prediction

The gradient boost method is used to create the prediction model. THe model is created using the selected variables and the training subset of the data. The model is saved as een .RDS object to use later for the quiz. 
```{r, echo= TRUE}
# create a prediction model using gradient boost method
set.seed(1235)
modFit <- train(classe ~. , method = "gbm", data = sub_train, verbose = FALSE)

saveRDS(modFit, "GBM_fit.Rds")
fittedModel <- readRDS("GBM_fit.Rds")

# predit the outcome with the test subset
predGBM <- predict(fittedModel, sub_test)
cm <- confusionMatrix(predGBM, sub_test$classe)
cm$overall['Accuracy']

# out of sample error
OOSE <- 1-cm$overall['Accuracy']
```

The out of sample error is calculated using the test subset. This subset is not used in the training of the model, only used for the prediction of the class. From the calculation, we can see that the prediction is very accurate, with an out of sample error of 0,425%. 
